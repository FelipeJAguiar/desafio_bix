{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4dc5b78",
   "metadata": {},
   "source": [
    "### 1. What steps would you take to solve this problem? Please describe as completely and clearly as possible all the steps that you see as essential for solving the problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec21257c",
   "metadata": {},
   "source": [
    "Elaborando a solução dentro da metodologia CRISP-DM, nos seguintes passos:\n",
    "\n",
    "1 - Business question: Analisar e entender as questões de negócios, quais as reais demandas do cliente e o que eles objetivam com a demanda especificadamente.\n",
    "\n",
    "2 - Business understanding: Realizar reuniões e contato com os clientes para entender melhor e detalhadamente a demanda do negócio a fim de chegar em conjunto a melhor solução viável para o time de DS e para os clientes; estabelecer as restrições de negócio relacionadas a demanda (base de dados disponíveis, informações técnicas, regras de negócio); buscar referenciais técnicos sobre o tema para embasar a solução; Entender a situação de momento e o que os levou a buscar pela solução, para assimilar melhor os requisitos do cliente; \n",
    "\n",
    "3 - Data collect: Identificar os meios e possibilidades de coleta de dados; Observar quais dados estão disponíveis e onde estão, e a melhor forma de coletá-los; \n",
    "\n",
    "4 - Data cleaning: Analisar as dimensões e fazer a descrição dos dados; avaliar a qualidade dos dados e fazer a limpeza e tratamento dos dados divergentes; excluir ou substituir \"NA's\"; checar os tipos das variáveis e fazer alterações necessárias; analisar se existe perda considerável de dados durante o tratamento;\n",
    "\n",
    "5 - Data Preparation: Realizar engenharia de features combinando, adicionando ou removendo features, ou até mesmo criando novos datasets com base no original;\n",
    "\n",
    "5 - Exploratory data analysis: Buscar por padrões e outliers das variáveis através de análises tabulares e gráficas (histogramas, bloxplots); realizar análises univariadas, bivariadas e multivariadas para entender o comportamento das variáveis e suas distribuições; fazer análises de corrrelação; gerar insights sobre os dados traduzidos para o negócio que auxiliem na tomada de decisão;  \n",
    "\n",
    "6 - Data modelling: Aplicar métodos ou algoritmos de seleção de features; realizar engenharia de features combinando, adicionando ou removendo features; aplicar transformações (normalização, reescala, encoding) para o melhor desempenho do modelo; selecionar os dados com base nas features mais importantes;\n",
    "\n",
    "7 - Machine learning algorithms: Aplicar os modelos de Machine Learning, como no caso usado, de classificação, como: Random Forest, Extra Tress, Logistic Regression, entre outros; ajustar hiperparâmetros e aplicar técnicas de cross-validation para obtenção de melhores resultados;\n",
    "\n",
    "8 - Algorithms evaluate: Metrificar e justificar os resultados obtidos; fazer a tradução dos resultados para os problemas de negócio, simplificando o entendimento de dados técnicos para pessoas não técnicas; quantificar os ganhos em relação ao momento atual, comparando e evidenciando os ganhos com a aplicação do modelo;\n",
    "\n",
    "9 - Production model: Realizar o deploy do modelo com base na demanda do cliente e infraestrutura do negócio; estipular critérios de manutenção do modelo; definir limiar de necessidade de retreino; construir doumentação do projeto;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f7e6b1",
   "metadata": {},
   "source": [
    "### 2. Which technical data science metric would you use to solve this challenge? Ex: absolute error, rmse, etc. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9558954",
   "metadata": {},
   "source": [
    "Por se tratar de um problema de classificação, eu escolheria as métricas relacionadas a esta técnica, como: Recall, Precision, Accuracy e F1-Score. Com foco em Recall e Precision para a tomada de decisão em relação direta ao negócio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b144fbf",
   "metadata": {},
   "source": [
    "### 3. Which business metric  would you use to solve the challenge?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84658bb3",
   "metadata": {},
   "source": [
    "Custo Total de Manutenção, Custo de Manutenção por caminhão"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f925e6",
   "metadata": {},
   "source": [
    "### 4. How do technical metrics relate to the business metrics?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ffe6ad",
   "metadata": {},
   "source": [
    "No caso estudado por exemplo, a métrica Precision resulta no quão preciso é o modelo em acertar se um caminhão tem defeito ou não. Já em relação a Recall, resulta em, entre os caminhões com defeito, o quanto conseguimos acertar que ele é defeituoso. Logo com base no modelo e nas métricas conseguimos prever se o caminhão é defeituoso  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf41c4cf",
   "metadata": {},
   "source": [
    "### 5. What types of analyzes would you like to perform on the customer database?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ee9844",
   "metadata": {},
   "source": [
    "Gostaria de saber mais sobre informações relacionadas a:\n",
    "\n",
    "Data de execução do serviço, para compreender melhor o comportamento dos serviços de manutenção ao longo do tempo (se existe sazonalidade e etc); \n",
    "\n",
    "O tipo de serviço que foi realizado (Preventivo, Preditivo ou Corretivo), para conseguir definir melhor o custo atual e este podendo ser um baseline mais preciso sobre a manutenção dos caminhões;\n",
    "\n",
    "Um identificador ('id') do caminhão, para entender quais caminhões necessitaram de cada tipo de manuntenção e quantos tipos foram necessários;\n",
    "\n",
    "Qual custo total de cada caminhão em relação a manuntenção desde a sua aquisição ou entrada na base de dados; \n",
    "\n",
    "Um identificador de motoristas que dirigiram o caminhão, para analisar se as falhas são correlatas aos motoristas ou não."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ee958f",
   "metadata": {},
   "source": [
    "### 6. What techniques would you use to reduce the dimensionality of the problem? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f874d8b",
   "metadata": {},
   "source": [
    "Poderiam ser usadas as técnicas de PCA, UMAP, t-SNE e Tree Based com Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9751f30",
   "metadata": {},
   "source": [
    "### 7. What techniques would you use to select variables for your predictive model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b0f3a3",
   "metadata": {},
   "source": [
    "Poderiam ser usadas tanto a Extra Trees quanto algoritmos como Boruta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0b1dce",
   "metadata": {},
   "source": [
    "### 8. What predictive models would you use or test for this problem? Please indicate at least 3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c7ed90",
   "metadata": {},
   "source": [
    "Extra Trees Classifier, Random Forest Classifier, XGBoost Classifier, Logistic Regression e KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdd0742",
   "metadata": {},
   "source": [
    "### 9. How would you rate which of the trained models is the best?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b992817",
   "metadata": {},
   "source": [
    "Através das métricas indicadas para os modelos de classificação, como AUC, F1-Score, Accuracy, Precision e Recall. Ou até mesmo gráficas como Matrizes de Confusão."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b961c6",
   "metadata": {},
   "source": [
    "### 10. How would you explain the result of your model? Is it possible to know which variables are most important?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c4dbae",
   "metadata": {},
   "source": [
    "Demonstraria os valores obtidos pelas métricas traduzindo-os em informações de negócio, como citado na pergunta \"4\". Sim, seria possível indicar as variáveis mais importantes através de uma análise exploratória bem estruturada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abb6680",
   "metadata": {},
   "source": [
    "### 11. How would you assess the financial impact of the proposed model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40f7f44",
   "metadata": {},
   "source": [
    "Realizando a comparação do custo total de manutenção antes e depois da implementação do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bdaab5",
   "metadata": {},
   "source": [
    "### 12. What techniques would you use to perform the hyperparameter optimization of the chosen model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b5b085",
   "metadata": {},
   "source": [
    "Uma das formas seria a utilização do Optuna, que realiza iterações de variados valores referentes aos parâmetros para buscar a melhor combinação que gera o melhor resultado para o modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d0ee94",
   "metadata": {},
   "source": [
    "### 13. What risks or precautions would you present to the customer before putting this model into production?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0b2619",
   "metadata": {},
   "source": [
    "Analisar cuidadosamente os resultados de Precision e Recall, pois eles refletem a estratégia do negócio para o problema, logo em qualquer alteração de estratégia é importante ajustar antes o modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e185375f",
   "metadata": {},
   "source": [
    "### 14. If your predictive model is approved, how would you put it into production?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6184f911",
   "metadata": {},
   "source": [
    "Implementação do modelo em um sistema de monitoramento contínuo e em tempo real, disponibilizando um modelo no servidor com o serviço de cron-job, abastecido pela base dados do cliente em tempo real, pré_programado para executar o modelo (diariamente, semanalmente ou mensalmente) e fornecer os dados de previsão conforme novos dados de manutenção de caminhões vão entrando.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9091c5f6",
   "metadata": {},
   "source": [
    "### 15. If the model is in production, how would you monitor it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e54a4df",
   "metadata": {},
   "source": [
    "Realizando o monitoramento contínuo das previsões do modelo. Comparação de previsões com resultados reais ao longo do tempo e caso necessário, estabelecer um plano de ajustes contínuos e re-treinamento do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703e0e2a",
   "metadata": {},
   "source": [
    "### 16. If the model is in production, how would you know when to retrain it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0075a2aa",
   "metadata": {},
   "source": [
    "Definir critérios e valores das métricas claros para quando o desempenho do modelo cair abaixo de um determinado limiar, re-treinar o modelo periódicamente e registrar seus resultados. Isso poderá indicar que novos dados apresentam tendência e padrões diferentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355d833b",
   "metadata": {},
   "source": [
    "# 0.0 Códigos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24db0616",
   "metadata": {},
   "source": [
    "## 0.1 IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c31a9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn import neighbors as nh\n",
    "from sklearn import linear_model as lm\n",
    "from sklearn import ensemble as en\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, precision_recall_curve, classification_report\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac24b3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Altteração de display \n",
    "pd.options.display.float_format = '{:.4f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136a55b5",
   "metadata": {},
   "source": [
    "## 0.2 Códigos auxiliares para as respostas do formulário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "265065b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../air_system_previous_years.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2437bde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de valores 'na' na coluna 'ee_003': 671\n"
     ]
    }
   ],
   "source": [
    "# Contar u número de 'na' na coluna\n",
    "na_count = df['ee_003'].value_counts().get('na', 0)\n",
    "print(f\"Quantidade de valores 'na' na coluna 'ee_003': {na_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cac21404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substituição de 'na' por 'NaN'\n",
    "df['ee_003'] = df['ee_003'].apply(lambda x: np.nan if x=='na' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "132a523a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtd valores nulos: 671\n"
     ]
    }
   ],
   "source": [
    "print('Qtd valores nulos:', df['ee_003'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e92b92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ee_003'] = df['ee_003'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfbd98f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Média de 'ee_003': 211126.44730233107\n",
      "Mediana de 'ee_003': 112086.0\n",
      "Desvio padrão de 'ee_003': 543318.816708597\n"
     ]
    }
   ],
   "source": [
    "mean_ee_003 = df['ee_003'].mean()\n",
    "median_ee_003 = df['ee_003'].median()\n",
    "std_ee_003 = df['ee_003'].std()\n",
    "\n",
    "print(f\"Média de 'ee_003': {mean_ee_003}\")\n",
    "print(f\"Mediana de 'ee_003': {median_ee_003}\")\n",
    "print(f\"Desvio padrão de 'ee_003': {std_ee_003}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6eb5f982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bcd8dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de valores 'na' na coluna 'ag_002': 671\n"
     ]
    }
   ],
   "source": [
    "na_count = df['ag_002'].value_counts().get('na', 0)\n",
    "print(f\"Quantidade de valores 'na' na coluna 'ag_002': {na_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4954f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ag_002'] = df['ag_002'].apply(lambda x: np.nan if x=='na' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4359d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtd valores nulos: 671\n"
     ]
    }
   ],
   "source": [
    "print('Qtd valores nulos:', df['ag_002'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ad1aeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ag_002'] = df['ag_002'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9261638",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['ag_002'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22b17420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtd valores nulos: 0\n"
     ]
    }
   ],
   "source": [
    "print('Qtd valores nulos:', df['ag_002'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "faef8495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Média de 'ag_002': 8606.014529151005\n",
      "Mediana de 'ag_002': 0.0\n",
      "Desvio padrão de 'ag_002': 150322.0285388566\n"
     ]
    }
   ],
   "source": [
    "mean_ee_003 = df['ag_002'].mean()\n",
    "median_ee_003 = df['ag_002'].median()\n",
    "std_ee_003 = df['ag_002'].std()\n",
    "\n",
    "print(f\"Média de 'ag_002': {mean_ee_003}\")\n",
    "print(f\"Mediana de 'ag_002': {median_ee_003}\")\n",
    "print(f\"Desvio padrão de 'ag_002': {std_ee_003}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbf2bb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a44d839",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ag_002'] = df['ag_002'].apply(lambda x: np.nan if x=='na' else x)\n",
    "df['ee_007'] = df['ee_007'].apply(lambda x: np.nan if x=='na' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6087420",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['ag_002','ee_007'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2ae6e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ag_002'] = df['ag_002'].astype(float)\n",
    "df['ee_007'] = df['ee_007'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e2d07a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ag_002  ee_007\n",
      "ag_002  1.0000  0.1951\n",
      "ee_007  0.1951  1.0000\n"
     ]
    }
   ],
   "source": [
    "correlation = df[['ag_002','ee_007']].corr('spearman')\n",
    "\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6996076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b8f1c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ac_000'] = df['ac_000'].apply(lambda x: np.nan if x=='na' else x)\n",
    "df['ee_005'] = df['ee_005'].apply(lambda x: np.nan if x=='na' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9501b337",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['ac_000','ee_005'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b4ece4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ac_000'] = df['ac_000'].astype(float)\n",
    "df['ee_005'] = df['ee_005'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2cd56ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ac_000  ee_005\n",
      "ac_000  1.0000 -0.0112\n",
      "ee_005 -0.0112  1.0000\n"
     ]
    }
   ],
   "source": [
    "correlation = df[['ac_000','ee_005']].corr('pearson')\n",
    "\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12f7cb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f6c1432",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ad_000'] = df['ad_000'].apply(lambda x: np.nan if x=='na' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4701cd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtd valores nulos: 11549\n"
     ]
    }
   ],
   "source": [
    "print('Qtd valores nulos:', df['ad_000'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "314a3758",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['ad_000'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9a82d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtd valores nulos: 0\n"
     ]
    }
   ],
   "source": [
    "print('Qtd valores nulos:', df['ad_000'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3578737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ad_000'] = df['ad_000'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b42011cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = df[['class','ad_000']].groupby('class').median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb828e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ad_000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>neg</th>\n",
       "      <td>126.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>581.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ad_000\n",
       "class         \n",
       "neg   126.0000\n",
       "pos   581.0000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d91f33fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "49db85b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtd valores nulos: 671\n"
     ]
    }
   ],
   "source": [
    "df['ee_001'] = df['ee_001'].apply(lambda x: np.nan if x=='na' else x)\n",
    "print('Qtd valores nulos:', df['ee_001'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d38846ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtd valores nulos: 0\n"
     ]
    }
   ],
   "source": [
    "df.dropna(subset=['ee_001'], inplace=True)\n",
    "print('Qtd valores nulos:', df['ee_001'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "417efa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ee_001'] = df['ee_001'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b664a264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ee_001</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>neg</th>\n",
       "      <td>661516.0093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>7957407.7387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ee_001\n",
       "class             \n",
       "neg    661516.0093\n",
       "pos   7957407.7387"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = df[['class','ee_001']].groupby('class').mean()\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1b78e126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a249d8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de valores 'na' na base de dados: 850015\n"
     ]
    }
   ],
   "source": [
    "count_na = df.isin(['na']).sum().sum()\n",
    "\n",
    "print(f\"Total de valores 'na' na base de dados: {count_na}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c30e6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "28a1399b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A coluna que está em 4º lugar é: bo_000\n"
     ]
    }
   ],
   "source": [
    "nc = df.isin(['na']).sum()\n",
    "\n",
    "nc_s = nc.sort_values(ascending=False)\n",
    "\n",
    "q_column = nc_s.index[3]  \n",
    "\n",
    "print(f\"A coluna que está em 4º lugar é: {q_column}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f17df72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "26ffa260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtd valores nulos: 160\n"
     ]
    }
   ],
   "source": [
    "df['ci_000'] = df['ci_000'].apply(lambda x: np.nan if x=='na' else x)\n",
    "print('Qtd valores nulos:', df['ci_000'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e8a697c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtd valores nulos: 0\n"
     ]
    }
   ],
   "source": [
    "df.dropna(subset=['ci_000'], inplace=True)\n",
    "print('Qtd valores nulos:', df['ci_000'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7d3dea02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ci_000'] = df['ci_000'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "79efb9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O valor do quantil 0.32 da variável 'ci_000' é: 98162.03520000003\n"
     ]
    }
   ],
   "source": [
    "quantile_032 = df['ci_000'].quantile(0.32)\n",
    "\n",
    "print(f\"O valor do quantil 0.32 da variável 'ci_000' é: {quantile_032}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "be834d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d31a67",
   "metadata": {},
   "source": [
    "# Código Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d624c0a",
   "metadata": {},
   "source": [
    "## 1.0 Data Collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000b9c22",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f3602c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('../filled_air_system_previous_years.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ad55ff",
   "metadata": {},
   "source": [
    "### Data cleaning and manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d259ed15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg    59000\n",
      "pos     1000\n",
      "Name: class, dtype: int64\n",
      "\n",
      "Falhas no sistema de ar representam 1.667% dos dados.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df1['class'].value_counts())\n",
    "print(\"\\nFalhas no sistema de ar representam {:.3f}% dos dados.\\n\".format((df1[df1['class'] == 'pos'].shape[0] / df1.shape[0]) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d7ecfc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alterando valores categóricos para númericos\n",
    "df1['class'] = df1['class'].apply(lambda x: 0 if x=='neg' else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c35dbfd",
   "metadata": {},
   "source": [
    "## 2.0 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1f0e0f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87da9614",
   "metadata": {},
   "source": [
    "### Dataset split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f017bd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar features e variável target\n",
    "X = df2.drop('class', axis=1)\n",
    "y = df2['class']\n",
    "\n",
    "# Separar o dataset em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509da58e",
   "metadata": {},
   "source": [
    "### Rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f283b87d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# Aplicar a reescala\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "\n",
    "# Aplicar a reescala ao conjunto de teste\n",
    "X_test_s = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9527582",
   "metadata": {},
   "source": [
    "### Data balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6dc48542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensão dos dados de treino: (94400, 170)\n",
      "Distribuição das classes nos dados de treino balanceado: 0    47200\n",
      "1    47200\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Aplicar SMOTE para balanceamento de dados\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_rs, y_train_rs = smote.fit_resample(X_train_s, y_train)\n",
    "\n",
    "# Checando as dimensões dos dados balanceados\n",
    "print(\"Dimensão dos dados de treino:\", X_train_rs.shape)\n",
    "print(\"Distribuição das classes nos dados de treino balanceado:\", y_train_rs.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "442a1829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformar em DataFrames para simplificar a seleção de colunas\n",
    "X_train_rs_df = pd.DataFrame(X_train_rs, columns=X.columns)\n",
    "X_test_s_df = pd.DataFrame(X_test_s, columns=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c6a328",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e1cfc0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking: \n",
      "    feature  importance\n",
      "23   ao_000      0.0311\n",
      "81   bu_000      0.0307\n",
      "68   bh_000      0.0293\n",
      "80   bt_000      0.0292\n",
      "67   bg_000      0.0255\n",
      "..      ...         ...\n",
      "139  dk_000      0.0000\n",
      "138  dj_000      0.0000\n",
      "141  dm_000      0.0000\n",
      "140  dl_000      0.0000\n",
      "93   ch_000      0.0000\n",
      "\n",
      "[170 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Definição do modelo\n",
    "forest = en.ExtraTreesClassifier(n_estimators=250, random_state=0, n_jobs=-1)\n",
    "\n",
    "# Treinar o modelo\n",
    "forest.fit(X_train_rs, y_train_rs)\n",
    "\n",
    "# Obter as features por importância\n",
    "importance = forest.feature_importances_\n",
    "indices = np.argsort(importance)[::-1]\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "\n",
    "# Criar df com as importâncias\n",
    "feature_names = df2.drop('class', axis=1).columns\n",
    "df_importance = pd.DataFrame({'feature': feature_names, 'importance': importance})\n",
    "\n",
    "# Ordenar e rankeamento das importâncias\n",
    "df_importance = df_importance.sort_values(by='importance', ascending=False)\n",
    "print(\"Feature ranking: \")\n",
    "print(df_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "62581139",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0311, 0.0307, 0.0293, 0.0292, 0.0255, 0.0255, 0.024 , 0.0239,\n",
       "       0.0236, 0.0235, 0.0234, 0.0234, 0.0219, 0.0218, 0.0193, 0.0187,\n",
       "       0.0178, 0.017 , 0.0166, 0.0151, 0.0149, 0.0144, 0.0142, 0.014 ,\n",
       "       0.0138, 0.0132, 0.0124, 0.0122, 0.0119, 0.0115, 0.0111, 0.0109,\n",
       "       0.0106, 0.0106, 0.0099, 0.0089, 0.0089, 0.0088, 0.0085, 0.0081,\n",
       "       0.0081, 0.0078, 0.0074, 0.0074, 0.0072, 0.0071, 0.007 , 0.007 ,\n",
       "       0.0069, 0.0069, 0.0067, 0.0062, 0.006 , 0.0057, 0.0054, 0.0053,\n",
       "       0.0052, 0.0049, 0.0049, 0.0049, 0.0048, 0.0048, 0.0047, 0.0046,\n",
       "       0.0046, 0.0046, 0.0045, 0.0044, 0.0042, 0.0041, 0.004 , 0.0038,\n",
       "       0.0037, 0.0036, 0.0034, 0.0033, 0.0033, 0.0032, 0.0031, 0.003 ,\n",
       "       0.003 , 0.0027, 0.0026, 0.0026, 0.0024, 0.0022, 0.0022, 0.0021,\n",
       "       0.0021, 0.002 , 0.002 , 0.0019, 0.0019, 0.0018, 0.0018, 0.0018,\n",
       "       0.0017, 0.0017, 0.0017, 0.0017, 0.0016, 0.0016, 0.0016, 0.0016,\n",
       "       0.0016, 0.0016, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n",
       "       0.0014, 0.0014, 0.0014, 0.0013, 0.0013, 0.0013, 0.0012, 0.0012,\n",
       "       0.0012, 0.0012, 0.0012, 0.0012, 0.0011, 0.0011, 0.0011, 0.0011,\n",
       "       0.0011, 0.0011, 0.0011, 0.001 , 0.001 , 0.001 , 0.001 , 0.001 ,\n",
       "       0.0009, 0.0009, 0.0009, 0.0008, 0.0008, 0.0008, 0.0007, 0.0007,\n",
       "       0.0007, 0.0007, 0.0007, 0.0006, 0.0005, 0.0005, 0.0005, 0.0005,\n",
       "       0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0001,\n",
       "       0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.    , 0.    , 0.    ,\n",
       "       0.    , 0.    ])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_importance['importance'].unique().round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f1e59bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionando as features mais importantes\n",
    "df_i = df_importance[df_importance['importance']>=0.004]\n",
    "# Obs.: o valor '0,004' foi definido de formar aleatória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5437b80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando os dados de treino com as features mais importantes\n",
    "X_train_rs1 = X_train_rs_df[df_i['feature'].unique()]\n",
    "X_test_s1 = X_test_s_df[df_i['feature'].unique()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e064ca7a",
   "metadata": {},
   "source": [
    "# 3.0 Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6f95d2",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f91babf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relatório de Classificação:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9983    0.9697    0.9838     11800\n",
      "           1     0.3358    0.9050    0.4899       200\n",
      "\n",
      "    accuracy                         0.9686     12000\n",
      "   macro avg     0.6671    0.9373    0.7368     12000\n",
      "weighted avg     0.9873    0.9686    0.9756     12000\n",
      "\n",
      "Acurácia: 0.9686\n",
      "\n",
      "AUC: 0.9373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_rs1, y_train_rs)\n",
    "\n",
    "y_pred = lr.predict(X_test_s1)\n",
    "y_proba = lr.predict_proba(X_test_s1)\n",
    "\n",
    "print(\"Relatório de Classificação:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "print(\"Acurácia: {:.4f}\\n\".format(accuracy_score(y_test, y_pred)))\n",
    "print(\"AUC: {:.4f}\\n\".format(roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e940f1",
   "metadata": {},
   "source": [
    "### Extra Trees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e925c7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relatório de Classificação:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9984    0.9894    0.9939     11800\n",
      "           1     0.5915    0.9050    0.7154       200\n",
      "\n",
      "    accuracy                         0.9880     12000\n",
      "   macro avg     0.7949    0.9472    0.8546     12000\n",
      "weighted avg     0.9916    0.9880    0.9892     12000\n",
      "\n",
      "Acurácia: 0.9880\n",
      "\n",
      "AUC: 0.9472\n",
      "\n"
     ]
    }
   ],
   "source": [
    "et = en.ExtraTreesClassifier(n_estimators=300, n_jobs=-1, random_state=42)\n",
    "et.fit(X_train_rs1, y_train_rs)\n",
    "\n",
    "y_pred = et.predict(X_test_s1)\n",
    "y_proba = et.predict_proba(X_test_s1)\n",
    "\n",
    "print(\"Relatório de Classificação:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "print(\"Acurácia: {:.4f}\\n\".format(accuracy_score(y_test, y_pred)))\n",
    "print(\"AUC: {:.4f}\\n\".format(roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e02477",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dc72b78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relatório de Classificação:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9976    0.9903    0.9939     11800\n",
      "           1     0.5993    0.8600    0.7064       200\n",
      "\n",
      "    accuracy                         0.9881     12000\n",
      "   macro avg     0.7985    0.9251    0.8501     12000\n",
      "weighted avg     0.9910    0.9881    0.9891     12000\n",
      "\n",
      "Acurácia: 0.9881\n",
      "\n",
      "AUC: 0.9251\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = en.RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42) \n",
    "rf.fit(X_train_rs1, y_train_rs)\n",
    "\n",
    "y_pred = rf.predict(X_test_s1)\n",
    "y_proba = rf.predict_proba(X_test_s1)\n",
    "\n",
    "print(\"Relatório de Classificação:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "print(\"Acurácia: {:.4f}\\n\".format(accuracy_score(y_test, y_pred)))\n",
    "print(\"AUC: {:.4f}\\n\".format(roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dc8b97",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0862888c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relatório de Classificação:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9989    0.9697    0.9841     11800\n",
      "           1     0.3438    0.9350    0.5027       200\n",
      "\n",
      "    accuracy                         0.9692     12000\n",
      "   macro avg     0.6713    0.9524    0.7434     12000\n",
      "weighted avg     0.9879    0.9692    0.9761     12000\n",
      "\n",
      "Acurácia: 0.9692\n",
      "\n",
      "AUC: 0.9524\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn = nh.KNeighborsClassifier(n_neighbors=7)\n",
    "knn.fit(X_train_rs1, y_train_rs)\n",
    "\n",
    "y_pred = knn.predict(X_test_s1)\n",
    "y_proba = knn.predict_proba(X_test_s1)\n",
    "\n",
    "print(\"Relatório de Classificação:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "print(\"Acurácia: {:.4f}\\n\".format(accuracy_score(y_test, y_pred)))\n",
    "print(\"AUC: {:.4f}\\n\".format(roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd79ae35",
   "metadata": {},
   "source": [
    "### XGBoost Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "82f2ada5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relatório de Classificação:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9989    0.9697    0.9841     11800\n",
      "           1     0.3438    0.9350    0.5027       200\n",
      "\n",
      "    accuracy                         0.9692     12000\n",
      "   macro avg     0.6713    0.9524    0.7434     12000\n",
      "weighted avg     0.9879    0.9692    0.9761     12000\n",
      "\n",
      "Acurácia: 0.9692\n",
      "\n",
      "AUC: 0.9524\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(n_estimators=100, eta=0.01, max_depth=10)\n",
    "xgb.fit(X_train_rs1, y_train_rs)\n",
    "\n",
    "y_pred = knn.predict(X_test_s1)\n",
    "y_proba = knn.predict_proba(X_test_s1)\n",
    "\n",
    "print(\"Relatório de Classificação:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "print(\"Acurácia: {:.4f}\\n\".format(accuracy_score(y_test, y_pred)))\n",
    "print(\"AUC: {:.4f}\\n\".format(roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a705a36",
   "metadata": {},
   "source": [
    "Os melhores modelos identificados foram: Random Forest e Extra Trees. Porém ainda apresentaram uma baixa pecisão em prever a 'classe 1' que são os caminhões que apresentaram problemas no sistema de ar, o que poderia resultar em várias manutenções corretivas no futuro, que são as de maior custo. Portanto é necessário ajustar o modelo para melhorar estes resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0544b183",
   "metadata": {},
   "source": [
    "## Thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6517f578",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "264ffa46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold: 0.72\n",
      "Precision: 0.7842105263157895\n",
      "Recall: 0.745\n",
      "F1-Score: 0.764102564102564\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9957    0.9965    0.9961     11800\n",
      "           1     0.7842    0.7450    0.7641       200\n",
      "\n",
      "    accuracy                         0.9923     12000\n",
      "   macro avg     0.8899    0.8708    0.8801     12000\n",
      "weighted avg     0.9922    0.9923    0.9922     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Obter as probabilidades para a classe positiva\n",
    "y_proba = rf.predict_proba(X_test_s1)[:, 1]\n",
    "\n",
    "# Calcular precision, recall e thresholds\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "\n",
    "# Encontrar o threshold que melhor equilibra precision e recall\n",
    "f1_scores = 2 * precision * recall / (precision + recall)\n",
    "best_index = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_index]\n",
    "\n",
    "print(f'Best Threshold: {best_threshold}')\n",
    "print(f'Precision: {precision[best_index]}')\n",
    "print(f'Recall: {recall[best_index]}')\n",
    "print(f'F1-Score: {f1_scores[best_index]}')\n",
    "\n",
    "# Aplicar o threshold escolhido para fazer previsões\n",
    "y_pred_adjusted = (y_proba >= best_threshold).astype(int)\n",
    "\n",
    "# Métricas com a aplicação do threshold\n",
    "print(classification_report(y_test, y_pred_adjusted, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f825289",
   "metadata": {},
   "source": [
    "### Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ba5ba5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold: 0.7\n",
      "Precision: 0.8167539267015707\n",
      "Recall: 0.78\n",
      "F1-Score: 0.7979539641943735\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9965    0.9966    0.9966     11800\n",
      "           1     0.7990    0.7950    0.7970       200\n",
      "\n",
      "    accuracy                         0.9932     12000\n",
      "   macro avg     0.8978    0.8958    0.8968     12000\n",
      "weighted avg     0.9932    0.9932    0.9932     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_proba = et.predict_proba(X_test_s1)[:, 1]\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "\n",
    "f1_scores = 2 * precision * recall / (precision + recall)\n",
    "best_index = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_index]\n",
    "\n",
    "print(f'Best Threshold: {best_threshold}')\n",
    "print(f'Precision: {precision[best_index]}')\n",
    "print(f'Recall: {recall[best_index]}')\n",
    "print(f'F1-Score: {f1_scores[best_index]}')\n",
    "\n",
    "# Aplicar o threshold escolhido para fazer previsões\n",
    "y_pred_adjusted = (y_proba >= best_threshold).astype(int)\n",
    "\n",
    "# Imprimir o relatório de classificação para o novo threshold\n",
    "print(classification_report(y_test, y_pred_adjusted, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eff493",
   "metadata": {},
   "source": [
    "O melhor resultado foi para o modelo de Extra Trees com um Threshold de 0,71. o Threshold funciona como um limiar, uma vez que cada probabilidade de falhar o sistema de ar é dada entre 1 e 0, ele estabelece que caminhões com uma probabilidade maior ou igual a 0,71 por exemplo, sejam classificados como propensos a falha no sistema. Esse valor pode ser ajustado manualmente também e pode ser um ferramenta de suporte de definição de qual manutenção fazer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dbc8cc",
   "metadata": {},
   "source": [
    "# 4.0 Apllying model in the recent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7e0c6bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_csv('../filled_air_system_present_years.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "76b44649",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['class'] = df_final['class'].apply(lambda x: 0 if x=='neg' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a12641a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    15625\n",
      "1      375\n",
      "Name: class, dtype: int64\n",
      "\n",
      "Falhas no sistema de ar representam 2.344% dos dados.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_final['class'].value_counts())\n",
    "print(\"\\nFalhas no sistema de ar representam {:.3f}% dos dados.\\n\".format((df_final[df_final['class'] == 1].shape[0] / df_final.shape[0]) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9e0745dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar features e a coluna alvo\n",
    "X_final = df_final.drop('class', axis=1)\n",
    "y_final = df_final['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a429d44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reescala\n",
    "scaler = StandardScaler()\n",
    "X_final_s = scaler.fit_transform(X_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "faeacd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformar em df\n",
    "X_final_s_df = pd.DataFrame(X_final_s, columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cd8f2706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleção de features importantes\n",
    "X_final_s1 = X_final_s_df[df_i['feature'].unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9d5638e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     15625\n",
      "           1       0.87      0.73      0.79       375\n",
      "\n",
      "    accuracy                           0.99     16000\n",
      "   macro avg       0.93      0.86      0.89     16000\n",
      "weighted avg       0.99      0.99      0.99     16000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fazer previsões nos novos dados\n",
    "y_new = et.predict_proba(X_final_s1)[:, 1]\n",
    "\n",
    "threshold = 0.63 \n",
    "y_pred = (y_new >= threshold).astype(int)  \n",
    "\n",
    "y_truef = y_final.copy()\n",
    "\n",
    "print(classification_report(y_truef, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41928162",
   "metadata": {},
   "source": [
    "Para os dados atuais (\"air_system_present_years.csv\"), o modelo obteve um desempenho semelhante aos dados de treino, o que comprova que generaliza bem para dados nunca vistos. Apresentou-se bons resultados para as métricas de Precision e Recall, ou seja, o modelo é consideravelmente bom em prever os caminhões que apresentam falhas e de todos caminhões que aprsentaram falhas eles conseguiu identificar bem a maioria delas.\n",
    "Cabe ressaltar a flexibilidade que o threshold dá para o negócio gerenciar o modelo de acordo com suas estratégias."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
